# Pandas Universal Patterns - Cheat Sheet

Die wichtigsten Patterns fÃ¼r DataFrame-Analysen

---

## ðŸŽ¯ Die 5 Must-Know Patterns

### 1. Duplikate finden & filtern

```python
# Alle Zeilen mit duplizierten Werten anzeigen
df[df.duplicated(subset='spalte', keep=False)]

# Gruppen mit mehr als N EintrÃ¤gen
counts = df.groupby('spalte').size()
df[df['spalte'].isin(counts[counts > N].index)]

# Alternative mit value_counts
vc = df['spalte'].value_counts()
df[df['spalte'].isin(vc[vc > N].index)]
```

### 2. Missing Values analysieren

```python
# Prozent Missing pro Spalte
missing_pct = 100 * df.isna().sum() / len(df)

# Zeilen mit NA in bestimmten Spalten
df[df['spalte'].isna()]

# Zeilen OHNE NA
df[df['spalte'].notna()]

# Spalten mit >X% Missing
missing_pct[missing_pct > X]
```

### 3. Groupby â†’ Filter â†’ Map zurÃ¼ck (UNIVERSAL PATTERN!)

```python
# Das Universal Pattern:
grouped = df.groupby('key').agg(...)      # 1. Gruppieren & aggregieren
filter_keys = grouped[bedingung].index    # 2. Keys mit Bedingung extrahieren
result = df[df['key'].isin(filter_keys)]  # 3. ZurÃ¼ck auf Original-DataFrame mappen

# Beispiel: HÃ¤user mit Durchschnittspreis > 500k
avg_price = df.groupby('house_id')['price'].mean()
expensive = avg_price[avg_price > 500000].index
df[df['house_id'].isin(expensive)]

# Beispiel: Kunden mit mehr als 10 Bestellungen
order_counts = df.groupby('customer_id').size()
frequent_customers = order_counts[order_counts > 10].index
df[df['customer_id'].isin(frequent_customers)]
```

### 4. Value Counts fÃ¼r Quick Analysis

```python
# Top N hÃ¤ufigste Werte
df['spalte'].value_counts().head(N)

# Nur Werte mit >X Vorkommen
vc = df['spalte'].value_counts()
frequent = vc[vc > X].index
df[df['spalte'].isin(frequent)]

# Verteilung in Prozent
df['spalte'].value_counts(normalize=True) * 100

# Mit Missing Values (NaN)
df['spalte'].value_counts(dropna=False)
```

### 5. Conditional Filtering kombinieren

```python
# Mehrere Bedingungen mit AND
df[(df['price'] > 100) & (df['bedrooms'] >= 3)]

# OR Bedingung
df[(df['city'] == 'Berlin') | (df['city'] == 'Munich')]

# .isin() fÃ¼r Listen (eleganter als viele OR)
df[df['city'].isin(['Berlin', 'Munich', 'Hamburg'])]

# NOT mit ~ (Tilde)
df[~df['city'].isin(['Berlin'])]

# Komplexe Bedingungen
df[(df['price'] > 100) &
   (df['bedrooms'] >= 3) &
   (df['city'].isin(['Berlin', 'Munich']))]
```

---

## ðŸ”¥ Standard-Reihenfolge fÃ¼r EDA

```python
# 1. ÃœBERBLICK VERSCHAFFEN
df.shape                      # (rows, columns)
df.info()                     # Datentypen, Memory, Non-Null Count
df.head(10)                   # Erste Zeilen ansehen
df.columns                    # Spaltennamen
df.dtypes                     # Datentypen pro Spalte

# 2. MISSING VALUES
df.isna().sum()                           # Count pro Spalte
missing_pct = 100 * df.isna().sum() / len(df)  # % pro Spalte
df[df.isna().any(axis=1)]                # Alle Zeilen mit mindestens einem NA

# 3. DUPLIKATE
df.duplicated().sum()                    # Wie viele Duplikate?
df[df.duplicated(keep=False)]            # Alle Duplikate anzeigen
df.drop_duplicates()                     # Duplikate entfernen

# 4. VERTEILUNGEN
df['spalte'].value_counts()              # FÃ¼r kategorische Daten
df['spalte'].describe()                  # FÃ¼r numerische Daten
df.groupby('key').size()                 # GruppengrÃ¶ÃŸe
df.groupby('key').size().describe()      # Statistik Ã¼ber GruppengrÃ¶ÃŸen

# 5. OUTLIERS & EXTREME VALUES
df['price'].quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])
df[df['price'] > df['price'].quantile(0.95)]  # Top 5%
df[df['price'] < df['price'].quantile(0.05)]  # Bottom 5%

# 6. KORRELATIONEN (numerisch)
df.corr()                                # Korrelationsmatrix
df.corr()['target_column'].sort_values(ascending=False)
```

---

## ðŸ’¡ Mental Models

### Pattern A: Komprimieren â†’ Filtern â†’ Expandieren

```python
# Viele Zeilen â†’ Wenige Zeilen â†’ Viele Zeilen
compressed = df.groupby('key').mean()              # DataFrame wird klein
filtered = compressed[compressed['value'] > X]      # Noch kleiner
expanded = df[df['key'].isin(filtered.index)]      # Wieder groÃŸ (Original-Zeilen)
```

### Pattern B: Boolean Masking

```python
# Bedingungen erstellen und kombinieren
mask1 = df['price'] > 100
mask2 = df['bedrooms'] >= 3
mask3 = df['city'] == 'Berlin'

# Kombinieren
combined_mask = mask1 & mask2 | mask3  # AND/OR kombinieren
result = df[combined_mask]

# Oder direkt:
result = df[(df['price'] > 100) & (df['bedrooms'] >= 3) | (df['city'] == 'Berlin')]
```

### Pattern C: Method Chaining

```python
# Mehrere Operationen hintereinander
result = (df
    .dropna(subset=['important_col'])           # NAs entfernen
    .query('price > 100 & bedrooms >= 3')       # Filtern
    .groupby('category')                         # Gruppieren
    .agg({'price': 'mean', 'quantity': 'sum'})  # Aggregieren
    .sort_values('price', ascending=False)       # Sortieren
    .head(10)                                    # Top 10
)
```

---

## ðŸ“‹ Quick Reference Table

| Task | Code | Output |
|------|------|--------|
| **Missing %** | `100 * df.isna().sum() / len(df)` | Series |
| **Duplikate** | `df.duplicated(subset='col', keep=False)` | Boolean Series |
| **Top N Werte** | `df['col'].value_counts().head(N)` | Series |
| **Gruppen filtern** | `df['key'].isin(df.groupby('key').size()[lambda x: x > 1].index)` | Boolean Series |
| **Outliers (Top 5%)** | `df[df['col'] > df['col'].quantile(0.95)]` | DataFrame |
| **Multiple AND** | `df[(cond1) & (cond2)]` | DataFrame |
| **Multiple OR** | `df[(cond1) | (cond2)]` | DataFrame |
| **NOT** | `df[~condition]` | DataFrame |
| **Unique Count** | `df['col'].nunique()` | int |
| **GruppengrÃ¶ÃŸe** | `df.groupby('key').size()` | Series |

---

## ðŸ” Wichtige Unterschiede

### `.isna()` vs `.isnull()`
- **Identisch!** Beide machen dasselbe
- Empfehlung: Nutze `.isna()` (moderner)

### `.size()` vs `.count()`
```python
df.groupby('key').size()    # ZÃ¤hlt alle Zeilen (inkl. NaN) â†’ Series
df.groupby('key').count()   # ZÃ¤hlt non-NA pro Spalte â†’ DataFrame
```

### `.duplicated()` Parameter
```python
df.duplicated(keep='first')   # Erstes behalten (Standard)
df.duplicated(keep='last')    # Letztes behalten
df.duplicated(keep=False)     # ALLE als True markieren
```

### `.value_counts()` vs `.groupby().size()`
```python
df['col'].value_counts()      # Sortiert nach HÃ¤ufigkeit (absteigend)
df.groupby('col').size()      # Sortiert nach Index (aufsteigend)
```

---

## ðŸŽ“ Advanced Patterns

### Lambda in Chains
```python
# Lambda fÃ¼r inline Filterung
df[df.groupby('key')['value'].transform('mean') > 100]

# Lambda in .agg()
df.groupby('key').agg(lambda x: x.max() - x.min())

# Lambda mit .filter()
df.groupby('key').filter(lambda x: len(x) > 5)
```

### Query fÃ¼r lesbare Filter
```python
# Statt:
df[(df['price'] > 100) & (df['bedrooms'] >= 3)]

# Nutze .query():
df.query('price > 100 and bedrooms >= 3')

# Mit Variablen:
min_price = 100
df.query('price > @min_price')
```

### Transform fÃ¼r Group-Operations
```python
# Durchschnitt pro Gruppe zu jeder Zeile hinzufÃ¼gen
df['group_avg'] = df.groupby('category')['price'].transform('mean')

# Zeilen Ã¼ber Gruppendurchschnitt
df[df['price'] > df.groupby('category')['price'].transform('mean')]
```

---

## âš¡ Performance Tipps

```python
# LANGSAM: .apply() mit Lambda
df['new'] = df['old'].apply(lambda x: x * 2)

# SCHNELL: Vectorized Operations
df['new'] = df['old'] * 2

# LANGSAM: Iteration Ã¼ber Zeilen
for idx, row in df.iterrows():
    df.at[idx, 'new'] = row['old'] * 2

# SCHNELL: Vectorized oder .apply() auf Spalten
df['new'] = df['old'] * 2
```

---

## ðŸ“Œ HÃ¤ufige Fehler vermeiden

```python
# âŒ FALSCH: Boolean ohne .index
counts = df.groupby('key').size() > 1
df[df['key'].isin(counts)]  # isin() sucht nach True/False!

# âœ… RICHTIG: .index verwenden
counts = df.groupby('key').size() > 1
df[df['key'].isin(counts[counts].index)]

# âŒ FALSCH: Einzelnes & statt &&
df[(df['a'] > 1) and (df['b'] < 5)]  # Error!

# âœ… RICHTIG: & verwenden
df[(df['a'] > 1) & (df['b'] < 5)]

# âŒ FALSCH: Assignment auf Slice
df[df['price'] > 100]['new_col'] = 'expensive'  # SettingWithCopyWarning!

# âœ… RICHTIG: .loc verwenden
df.loc[df['price'] > 100, 'new_col'] = 'expensive'
```

---

## ðŸ’¾ Datentypen

| Python | NumPy Array | Pandas Series |
|--------|-------------|---------------|
| Built-in | Schnell, homogen | Schnell, mit Index |
| Gemischte Typen âœ… | Nur 1 Typ | Nur 1 Typ |
| Position Index | Position Index | **Label + Position** |
| `[1, 2, 3]` | `np.array([1,2,3])` | `pd.Series([1,2,3])` |
| Langsam (loops) | Sehr schnell (vectorized) | Sehr schnell |

### Wann was?
- **List**: Allgemeine Daten, gemischte Typen, klein
- **NumPy Array**: Mathematik, ML, groÃŸe numerische Daten
- **Pandas Series**: Eine Spalte aus DataFrame, Labels wichtig

---

**Erstellt: 2026-01-01**
**FÃ¼r: EDA & Data Analysis mit Pandas**
